# Performance Optimization Guidelines

## üöÄ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ Google Drive Smart Uploader Bot

### 1. OCR –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

#### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö PDF —Ñ–∞–π–ª–æ–≤

```python
# ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
async def extract_text_slow(pdf_path: str) -> str:
    with fitz.open(pdf_path) as doc:
        text = ""
        for page in doc:
            text += page.get_text()
    return text
```

```python
# ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Optional

class OptimizedOCR:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.cache = {}
    
    async def extract_text_optimized(self, pdf_path: str) -> str:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{pdf_path}:{os.path.getmtime(pdf_path)}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        pages_text = await self._process_pages_parallel(pdf_path)
        result = "\n".join(pages_text)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.cache[cache_key] = result
        return result
    
    async def _process_pages_parallel(self, pdf_path: str) -> List[str]:
        loop = asyncio.get_event_loop()
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        page_count = await loop.run_in_executor(
            self.executor, self._get_page_count, pdf_path
        )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = []
        for page_num in range(page_count):
            task = loop.run_in_executor(
                self.executor, self._extract_page_text, pdf_path, page_num
            )
            tasks.append(task)
        
        return await asyncio.gather(*tasks)
    
    def _extract_page_text(self, pdf_path: str, page_num: int) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è)"""
        with fitz.open(pdf_path) as doc:
            page = doc[page_num]
            return page.get_text()
```

### 2. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Redis

```python
import redis.asyncio as redis
import orjson
from typing import Optional, Any

class CacheManager:
    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1 —á–∞—Å
    
    async def get(self, key: str) -> Optional[Any]:
        try:
            data = await self.redis.get(key)
            if data:
                return orjson.loads(data)
        except Exception as e:
            log.error("cache_get_error", key=key, error=str(e))
        return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        try:
            ttl = ttl or self.default_ttl
            data = orjson.dumps(value)
            await self.redis.setex(key, ttl, data)
            return True
        except Exception as e:
            log.error("cache_set_error", key=key, error=str(e))
            return False
    
    async def invalidate(self, pattern: str) -> int:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–π –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        keys = await self.redis.keys(pattern)
        if keys:
            return await self.redis.delete(*keys)
        return 0

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞
cache = CacheManager(settings.redis_dsn)

async def get_ocr_result_cached(file_id: str, file_hash: str) -> str:
    cache_key = f"ocr:{file_id}:{file_hash}"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached_result = await cache.get(cache_key)
    if cached_result:
        return cached_result
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º OCR
    result = await extract_text_optimized(file_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –Ω–∞ 24 —á–∞—Å–∞
    await cache.set(cache_key, result, ttl=86400)
    
    return result
```

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤

```python
import aiofiles
from typing import AsyncGenerator

class StreamingUploader:
    def __init__(self, chunk_size: int = 8192):
        self.chunk_size = chunk_size
    
    async def upload_large_file(self, file_path: str, drive_service) -> str:
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        file_size = os.path.getsize(file_path)
        
        if file_size > 50 * 1024 * 1024:  # > 50MB
            return await self._resumable_upload(file_path, drive_service)
        else:
            return await self._simple_upload(file_path, drive_service)
    
    async def _resumable_upload(self, file_path: str, drive_service) -> str:
        """Resumable upload –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        media = MediaFileUpload(
            file_path,
            mimetype='application/octet-stream',
            resumable=True,
            chunksize=1024 * 1024  # 1MB chunks
        )
        
        request = drive_service.files().create(
            body={'name': os.path.basename(file_path)},
            media_body=media
        )
        
        response = None
        while response is None:
            status, response = request.next_chunk()
            if status:
                log.info("upload_progress", progress=status.progress() * 100)
        
        return response['id']
    
    async def _simple_upload(self, file_path: str, drive_service) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        async with aiofiles.open(file_path, 'rb') as f:
            content = await f.read()
        
        media = MediaIoBaseUpload(
            io.BytesIO(content),
            mimetype='application/octet-stream'
        )
        
        result = drive_service.files().create(
            body={'name': os.path.basename(file_path)},
            media_body=media
        ).execute()
        
        return result['id']
```

### 4. Memory Management

```python
import gc
import psutil
from contextlib import asynccontextmanager

class MemoryManager:
    def __init__(self, max_memory_mb: int = 500):
        self.max_memory_mb = max_memory_mb
    
    def get_memory_usage(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ MB"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    @asynccontextmanager
    async def memory_limit(self, operation_name: str):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞–º—è—Ç–∏"""
        initial_memory = self.get_memory_usage()
        
        try:
            yield
        finally:
            final_memory = self.get_memory_usage()
            memory_diff = final_memory - initial_memory
            
            log.info(
                "memory_usage",
                operation=operation_name,
                initial_mb=initial_memory,
                final_mb=final_memory,
                diff_mb=memory_diff
            )
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –º–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            if final_memory > self.max_memory_mb:
                gc.collect()
                log.warning("memory_cleanup_forced", memory_mb=final_memory)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
memory_manager = MemoryManager()

async def process_large_file(file_path: str) -> str:
    async with memory_manager.memory_limit("large_file_processing"):
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏
        result = await extract_text_optimized(file_path)
        return result
```

### 5. Batch Processing Optimization

```python
from dataclasses import dataclass
from typing import List, Dict, Any
import asyncio

@dataclass
class BatchJob:
    file_id: str
    file_path: str
    priority: int = 1
    metadata: Dict[str, Any] = None

class BatchProcessor:
    def __init__(self, max_concurrent: int = 5):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.queue = asyncio.Queue()
        self.workers = []
    
    async def start_workers(self):
        """–ó–∞–ø—É—Å–∫ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏"""
        for i in range(self.max_concurrent):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)
    
    async def stop_workers(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–æ—Ä–∫–µ—Ä–æ–≤"""
        for worker in self.workers:
            worker.cancel()
        await asyncio.gather(*self.workers, return_exceptions=True)
    
    async def _worker(self, worker_name: str):
        """–í–æ—Ä–∫–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞–Ω–∏–π"""
        while True:
            try:
                job = await self.queue.get()
                
                async with self.semaphore:
                    await self._process_job(job, worker_name)
                
                self.queue.task_done()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                log.error("worker_error", worker=worker_name, error=str(e))
    
    async def _process_job(self, job: BatchJob, worker_name: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è"""
        start_time = time.time()
        
        try:
            log.info("job_started", job_id=job.file_id, worker=worker_name)
            
            # OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞
            text = await get_ocr_result_cached(job.file_id, job.metadata.get('hash'))
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Drive
            drive_id = await upload_to_drive(job.file_path, text)
            
            processing_time = time.time() - start_time
            log.info(
                "job_completed",
                job_id=job.file_id,
                worker=worker_name,
                processing_time=processing_time,
                drive_id=drive_id
            )
            
        except Exception as e:
            log.error("job_failed", job_id=job.file_id, worker=worker_name, error=str(e))
            raise
    
    async def add_job(self, job: BatchJob):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–¥–∞–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å"""
        await self.queue.put(job)
    
    async def wait_completion(self):
        """–ñ–¥–∞—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞–Ω–∏–π"""
        await self.queue.join()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
batch_processor = BatchProcessor(max_concurrent=3)

async def process_files_batch(files: List[str]):
    await batch_processor.start_workers()
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞–Ω–∏—è
        for i, file_path in enumerate(files):
            job = BatchJob(
                file_id=f"file_{i}",
                file_path=file_path,
                priority=1,
                metadata={'hash': calculate_file_hash(file_path)}
            )
            await batch_processor.add_job(job)
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        await batch_processor.wait_completion()
        
    finally:
        await batch_processor.stop_workers()
```

### 6. Database Connection Pooling

```python
import asyncpg
from contextlib import asynccontextmanager

class DatabasePool:
    def __init__(self, dsn: str, min_size: int = 5, max_size: int = 20):
        self.dsn = dsn
        self.min_size = min_size
        self.max_size = max_size
        self.pool = None
    
    async def init_pool(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        self.pool = await asyncpg.create_pool(
            self.dsn,
            min_size=self.min_size,
            max_size=self.max_size,
            command_timeout=60
        )
    
    async def close_pool(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if self.pool:
            await self.pool.close()
    
    @asynccontextmanager
    async def acquire(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑ –ø—É–ª–∞"""
        async with self.pool.acquire() as connection:
            yield connection
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
import time
from functools import wraps

def monitor_performance(operation_name: str):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = memory_manager.get_memory_usage()
            
            try:
                result = await func(*args, **kwargs)
                
                execution_time = time.time() - start_time
                end_memory = memory_manager.get_memory_usage()
                memory_diff = end_memory - start_memory
                
                log.info(
                    "performance_metrics",
                    operation=operation_name,
                    execution_time=execution_time,
                    memory_usage_mb=end_memory,
                    memory_diff_mb=memory_diff
                )
                
                return result
                
            except Exception as e:
                execution_time = time.time() - start_time
                log.error(
                    "performance_error",
                    operation=operation_name,
                    execution_time=execution_time,
                    error=str(e)
                )
                raise
        
        return wrapper
    return decorator

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
@monitor_performance("ocr_processing")
async def extract_text_monitored(file_path: str) -> str:
    return await extract_text_optimized(file_path)
```

## üéØ Performance Checklist

### –ü–µ—Ä–µ–¥ –¥–µ–ø–ª–æ–µ–º:
- [ ] –ù–∞—Å—Ç—Ä–æ–µ–Ω Redis –∫—ç—à –¥–ª—è OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω connection pooling
- [ ] –î–æ–±–∞–≤–ª–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞–≥—Ä—É–∑–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
- [ ] –ù–∞—Å—Ç—Ä–æ–µ–Ω batch processing

### –†–µ–≥—É–ª—è—Ä–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:
- [ ] –í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞ API
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
- [ ] –†–∞–∑–º–µ—Ä –∫—ç—à–∞ Redis
- [ ] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
- [ ] –ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å

---

**–¶–µ–ª—å: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ < 5 —Å–µ–∫ –¥–ª—è —Ñ–∞–π–ª–æ–≤ < 10MB, < 30 —Å–µ–∫ –¥–ª—è —Ñ–∞–π–ª–æ–≤ < 50MB**